{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of bands are too small (b < 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m sample_dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Trova le frasi più simili solo sul campione rispetto al dataset completo\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mfind_most_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_perm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Stampa i risultati\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m original, similar \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "Cell \u001b[1;32mIn[1], line 40\u001b[0m, in \u001b[0;36mfind_most_similar\u001b[1;34m(dataset, sample_dataset, num_perm, threshold)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Costruzione di MinHash e LSH per l'intero dataset\u001b[39;00m\n\u001b[0;32m     39\u001b[0m minhashes \u001b[38;5;241m=\u001b[39m compute_minhash(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence_processed\u001b[39m\u001b[38;5;124m'\u001b[39m], num_perm\u001b[38;5;241m=\u001b[39mnum_perm)\n\u001b[1;32m---> 40\u001b[0m lsh \u001b[38;5;241m=\u001b[39m \u001b[43mMinHashLSH\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_perm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_perm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Aggiunge MinHash al LSH per l'intero dataset\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, mh \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(minhashes):\n",
      "File \u001b[1;32mc:\\Users\\saver\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasketch\\lsh.py:174\u001b[0m, in \u001b[0;36mMinHashLSH.__init__\u001b[1;34m(self, threshold, num_perm, weights, params, storage_config, prepickle, hashfunc)\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mr \u001b[38;5;241m=\u001b[39m _optimal_param(\n\u001b[0;32m    171\u001b[0m         threshold, num_perm, false_positive_weight, false_negative_weight\n\u001b[0;32m    172\u001b[0m     )\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of bands are too small (b < 2)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepickle \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    177\u001b[0m     storage_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mredis\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m prepickle \u001b[38;5;28;01melse\u001b[39;00m prepickle\n\u001b[0;32m    178\u001b[0m )\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhashfunc \u001b[38;5;241m=\u001b[39m hashfunc\n",
      "\u001b[1;31mValueError\u001b[0m: The number of bands are too small (b < 2)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "import string\n",
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocessa una frase rimuovendo punteggiatura e rendendo il testo in minuscolo.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def compute_minhash(sentences, num_perm=128):\n",
    "    \"\"\"Calcola MinHash per una lista di frasi.\"\"\"\n",
    "    minhashes = []\n",
    "    for sentence in sentences:\n",
    "        m = MinHash(num_perm=num_perm)\n",
    "        for word in sentence.split():\n",
    "            m.update(word.encode('utf8'))\n",
    "        minhashes.append(m)\n",
    "    return minhashes\n",
    "\n",
    "def find_most_similar(dataset, sample_dataset, num_perm=128, threshold=0.8):\n",
    "    \"\"\"\n",
    "    Trova la frase più simile per ogni frase nel campione all'interno del dataset completo usando LSH.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset: DataFrame contenente `lan_code` e `sentence` (dataset completo)\n",
    "    - sample_dataset: DataFrame contenente il campione di frasi\n",
    "    - num_perm: Numero di permutazioni per MinHash\n",
    "    - threshold: Soglia per la similarità LSH\n",
    "    \"\"\"\n",
    "    # Preprocessing del testo\n",
    "    dataset['sentence_processed'] = dataset['sentence'].apply(preprocess_text)\n",
    "    sample_dataset['sentence_processed'] = sample_dataset['sentence'].apply(preprocess_text)\n",
    "\n",
    "    # Costruzione di MinHash e LSH per l'intero dataset\n",
    "    minhashes = compute_minhash(dataset['sentence_processed'], num_perm=num_perm)\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "\n",
    "    # Aggiunge MinHash al LSH per l'intero dataset\n",
    "    for i, mh in enumerate(minhashes):\n",
    "        lsh.insert(f\"sentence_{i}\", mh)\n",
    "\n",
    "    # Ricerca della frase più simile nel dataset completo per ogni frase nel campione\n",
    "    similar_sentences = []\n",
    "    for i, row in sample_dataset.iterrows():\n",
    "        original_sentence = row['sentence_processed']\n",
    "        mh = MinHash(num_perm=num_perm)\n",
    "        for word in original_sentence.split():\n",
    "            mh.update(word.encode('utf8'))\n",
    "\n",
    "        # Trova frasi simili usando LSH\n",
    "        results = lsh.query(mh)\n",
    "\n",
    "        if results:\n",
    "            # Esclude la stessa frase (se presente)\n",
    "            results = [r for r in results if r != f\"sentence_{i}\"]\n",
    "\n",
    "            # Calcola la frase più simile tra i risultati usando cosine similarity\n",
    "            candidate_indices = [int(r.split(\"_\")[1]) for r in results]\n",
    "            candidates = dataset.iloc[candidate_indices]['sentence_processed']\n",
    "\n",
    "            vectorizer = TfidfVectorizer().fit([original_sentence] + candidates.tolist())\n",
    "            vectors = vectorizer.transform([original_sentence] + candidates.tolist())\n",
    "            similarities = cosine_similarity(vectors[0:1], vectors[1:]).flatten()\n",
    "            best_match_index = candidate_indices[similarities.argmax()]\n",
    "            similar_sentences.append((row['sentence'], dataset.iloc[best_match_index]['sentence']))\n",
    "        else:\n",
    "            similar_sentences.append((row['sentence'], None))\n",
    "\n",
    "    return similar_sentences\n",
    "\n",
    "# Esempio di utilizzo\n",
    "if __name__ == \"__main__\":\n",
    "    # Carica il dataset\n",
    "    file_path = \"data/filtered_language_detection.csv\"  # Sostituisci con il percorso del file CSV\n",
    "    dataset = pd.read_csv(file_path)\n",
    "\n",
    "    # Preleva lo 0.1% del dataset casualmente\n",
    "    sample_dataset = dataset.sample(frac=0.001, random_state=42)\n",
    "\n",
    "    # Trova le frasi più simili solo sul campione rispetto al dataset completo\n",
    "    results = find_most_similar(dataset, sample_dataset, num_perm=128, threshold=0.99)\n",
    "\n",
    "    # Stampa i risultati\n",
    "    for original, similar in results:\n",
    "        print(f\"Original: {original}\\nMost similar: {similar}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
